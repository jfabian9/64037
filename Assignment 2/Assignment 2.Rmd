---
title: "Assignment 2"
author: "Jacob Fabian"
date: "2023-04-08"
output:
  pdf_document: default
  html_document: default
---

#Q1
What  is  the  key  idea  behind  bagging?  Can  bagging  deal  both  with  high  variance (overfitting) and high bias (underfitting)?

#A1
The main concept behind bagging is that we reduce the variance by randomly selecting samples of the training set and using the replacement, then each weak learner is fitted on each sample data. This is used to prevent overfitting and would not be an option for underfitting.

#Q2
Why bagging models are computationally more efficient when compared to boosting 
models with the same number of weak learners?

#A2
In Boosting, models are continuously grown and each model is grown using information from the previousl model, there is no sequential growth in bagging which makes boosting more efficient. 

#Q3
James is thinking of creating an ensemble mode to predict whether a given stock will go up or down in the next week. He has trained several decision tree models but each model is not performing any better than a random model. The models are also very similar to each other. Do you think creating an ensemble model by combining these tree models can boost the performance?

#A3
The big issue is James has made multiple decision tree models that are too similar. The ensemble method of boosting would help in this situation as the mistakes from previous models can be used to make the next models better. It is important to remember that having dissimilar trees are important as it helps create a more robust model. I would go back to the start and rebuild the decision trees to be further more different from each other by using a punishment for selecting a certain attribute at a specific level too many times. 

#Q4
Consider the following Table that classifies some objects into two classes of edible (+) and non- edible (-), based on some characteristics such as the object color, size and shape. What would be the Information gain for splitting the dataset based on the “Size” attribute? 

#A4
Information gain = entropy(parent) - [avg. entropy (children)]

With the data provided the
parent entropy is = 0.988699
small size entropy is = 0.811278
large size entropy is = 0.954434

We can determine by using the calculation that the Information Gain is 0.105843

#Q5
Why is it important that the m parameter (number of attributes available at each split) to be optimally set in random forest models? Discuss the implications of setting this parameter too small or too large.

#A5
If the parameter is too large then it wouldn't be different than using bagging and we wouldn't get diversity. If the parameter is set too small, each tree won't be very predictive since they will be constrained at each node to a small part of attributes. It is important to allow random forests to be optimally set as the key part in random forests so that at each node, a randomly sample of predictor are used so that not every node is similar and a more accurate predictor will be the result.

# Part B
This part of the assignment involves building decision tree and random forest models to answer a number of questions. We will use the Carseats dataset that is part of the ISLR package. 

```{r}
library(caret)
library(ISLR)
library(glmnet)
library(dplyr)
library(rpart)
Carseats_Filtered <- Carseats %>% select("Sales", "Price", "Advertising", "Population", "Age", "Income", "Education")
```

#Q1
Build  a  decision  tree  regression  model  to  predict  Sales  based  on  all  other attributes ("Price", "Advertising", "Population", "Age", "Income" and "Education").  Which attribute is used at the top of the tree (the root node) for splitting?

```{r}
Model_1 <- rpart(Sales~., data=Carseats_Filtered, method = 'anova')
plot(Model_1)
text(Model_1)
```
Price greater than or equal to 94.5 is our root node for splitting

#Q2
Consider the following input - Sales=9, Price=6.54, Population=124, Advertising=0, Age=76, Income= 110, Education=10 What will be the estimated Sales for this record using the decision tree model?

```{r}
Sales <- c(9)
Price <- c(6.54)
Population <- c(124)
Advertising <- c(0)
Age <- c(76)
Income <- c(110)
Education <- c(10)
Test <- data.frame(Sales,Price,Population,Advertising,Age,Income,Education)
```


Predicting our sales since we have our test set to run with our model


```{r}
Pred_sales_2 <- predict(Model_1, Test)
Pred_sales_2
```

After running our predict function, the decision tree indicates that 9.58625 sales will take place

#Q3
Use the caret function to train a random forest (method=’rf’) for the same dataset. Use the caret default settings. By default, caret will examine the “mtry” values of 2,4, and 6. Recall that mtry is the number of attributes available for splitting at each splitting node. Which mtry value gives the best performance?


```{r}
set.seed(123)
Model_forest_caret <- train(Sales~., data = Carseats_Filtered, method = 'rf')
```

```{r}
summary(Model_forest_caret)
print(Model_forest_caret)
plot(Model_forest_caret)
```


The mtry with the lowest RMSE is 2, this is the best fit for mtry 

#Q4
Customize the search grid by checking the model’s performance for mtry values of 2, 
3 and 5 using 3 repeats of 5-fold cross validation.



```{r}
control <- trainControl(method="repeatedcv", number=5, repeats=3, search="grid")
tunegrid <- expand.grid(.mtry=c(2,3,5))
rf_gridsearch <- train(Sales~., data=Carseats_Filtered, method="rf", tuneGrid=tunegrid,trControl=control)
print(rf_gridsearch)
plot(rf_gridsearch)
```

While using 5 fold cross validation, and checking mtry at 2,3, and 5 with 3 repeats. We find that 2 mtry is the ideal mtry with the lowest RMSE of 2.388490 
